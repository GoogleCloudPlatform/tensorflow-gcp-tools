{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HtI6H6dxcvEn"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dC1ewu-RcvEs"
      },
      "source": [
        "\u003ctable align=\"left\"\u003e\n",
        "    \u003ctd\u003e\n",
        "        \u003ca href=\"https://console.cloud.google.com/mlengine/notebooks/deploy-notebook?q=download_url%3Dhttps://github.com/GoogleCloudPlatform/tensorflow-gcp-tools/blob/master/examples/cloud_fit.ipynb\"\u003e\n",
        "            \u003cimg src=\"https://www.gstatic.com/images/branding/product/1x/google_cloud_48dp.png\" alt=\"AI Platform Notebooks\"\u003e Run in AI Platform Notebooks\n",
        "        \u003c/a\u003e\n",
        "    \u003c/td\u003e\n",
        "    \u003ctd\u003e\n",
        "        \u003ca href=\"https://colab.research.google.com/github/GoogleCloudPlatform/tensorflow-gcp-tools/blob/master/examples/cloud_fit.ipynb\"\u003e\n",
        "            \u003cimg src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"\u003e Run in Colab\n",
        "        \u003c/a\u003e\n",
        "    \u003c/td\u003e\n",
        "    \u003ctd\u003e\n",
        "        \u003ca href=\"https://github.com/GoogleCloudPlatform/tensorflow-gcp-tools/blob/master/examples/cloud_fit.ipynb\"\u003e\n",
        "            \u003cimg src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"\u003eView on GitHub\n",
        "        \u003c/a\u003e\n",
        "     \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R2-BJ_QbcvEs"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Following is a quick introduction to *cloud_fit*. *cloud_fit* enables training on Google Cloud AI Platform in the same manner as model.fit().\n",
        "In this notebook, we will start by installing libraries required, then proceed with two samples showing how to use Numpy.array and TF.data.dataset with cloud_fit\n",
        "\n",
        "\n",
        "### What are the components of the cloud_fit()\n",
        "\n",
        "Cloud fit has two main components as follows:\n",
        "\n",
        "**client.py:** serializes the provided data and model along with typical model.fit() parameters and triggers a AI platform training\n",
        "``` python\n",
        "def cloud_fit(model,\n",
        "              remote_dir: Text,\n",
        "              region: Text = None,\n",
        "              project_id: Text = None,\n",
        "              image_uri: Text = None,\n",
        "              distribution_strategy: Text = DEFAULT_DISTRIBUTION_STRATEGY,\n",
        "              job_spec: Dict[str, Any] = None,\n",
        "              **fit_kwargs) -\u003e Text:\n",
        "  \"\"\"Facilitates remote execution of in memory Models and Datasets on AI Platform.\n",
        "\n",
        "  Args:\n",
        "    model: A compiled Keras Model.\n",
        "    remote_dir: Google Cloud Storage path for temporary assets and AI Platform\n",
        "      training output. Will overwrite value in job_spec.\n",
        "    region: Target region for running the AI Platform Training job.\n",
        "    project_id: Project id where the training should be deployed to.\n",
        "    image_uri: base image used to use for AI Platform Training\n",
        "    distribution_strategy: Specifies the distribution strategy for remote\n",
        "      execution when a jobspec is provided. Accepted values are strategy names\n",
        "      as specified by 'tf.distribute.\u003cstrategy\u003e.__name__'.\n",
        "    job_spec: AI Platform training job_spec, will take precedence over all other\n",
        "      provided values except for remote_dir. If none is provided a default\n",
        "      cluster spec and distribution strategy will be used.\n",
        "    **fit_kwargs: Args to pass to model.fit() including training and eval data.\n",
        "      Only keyword arguments are supported. Callback functions will be\n",
        "      serialized as is.\n",
        "\n",
        "  Returns:\n",
        "    AI Platform job ID\n",
        "\n",
        "  Raises:\n",
        "    RuntimeError: If executing in graph mode, eager execution is required for\n",
        "    cloud_fit.\n",
        "    NotImplementedError: Tensorflow v1.x is not supported.\n",
        "  \"\"\"\n",
        "```\n",
        "\n",
        "**remote.py:** A job that takes in a remote_dir as parameter , load model and data from this location and executes the training with stored parameters.\n",
        "```python\n",
        "def run(remote_dir: Text, distribution_strategy_text: Text):\n",
        "  \"\"\"deserializes Model and Dataset and runs them.\n",
        "\n",
        "  Args:\n",
        "    remote_dir: Temporary cloud storage folder that contains model and Dataset\n",
        "      graph. This folder is also used for job output.\n",
        "    distribution_strategy_text: Specifies the distribution strategy for remote\n",
        "      execution when a jobspec is provided. Accepted values are strategy names\n",
        "      as specified by 'tf.distribute.\u003cstrategy\u003e.__name__'.\n",
        "  \"\"\"\n",
        "```\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* AI Platform Training\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [AI Platform Training\n",
        "pricing](https://cloud.google.com/ai-platform/training/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QwMvZmuucvEt"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project.](https://console.cloud.google.com/cloud-resource-manager) When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the AI Platform APIs](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com)\n",
        "\n",
        "4. If running locally on your own machine, you will need to install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BxnMQKyDcvEt"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using [AI Platform Notebooks](https://cloud.google.com/ai-platform/notebooks/docs/)**, your environment is already\n",
        "authenticated. Skip these steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8ztCXRy5cvEu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your Google Cloud account. This provides access\n",
        "# to your Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this tutorial in a notebook locally, replace the string\n",
        "# below with the path to your service account key and run this cell to\n",
        "# authenticate your Google Cloud account.\n",
        "else:\n",
        "    %env GOOGLE_APPLICATION_CREDENTIALS your_path_to_credentials.json\n",
        "\n",
        "# Log in to your account on Google Cloud\n",
        "! gcloud auth application-default login --quiet\n",
        "! gcloud auth login --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z8GvrsnmcvEx"
      },
      "source": [
        "## Clone and build tensorflow_enterprise_addons\n",
        "\n",
        "To use the latest version of the tensorflow_enterprise_addons, we will clone and build the repo. The resulting whl file is both used in the client side as well as in construction of a docker image for remote execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "O4gedCSZcvEy"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/GoogleCloudPlatform/tensorflow-gcp-tools.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zafJjYrXcvE1"
      },
      "outputs": [],
      "source": [
        "!cd tensorflow-gcp-tools/python \u0026\u0026 python3 setup.py -q bdist_wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tPOgkeSncvE4"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow-gcp-tools/python/dist/tensorflow_enterprise_addons-*.whl --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LyF5aOg0cvE6"
      },
      "source": [
        "#### Restart the Kernel\n",
        "\n",
        "We will automatically restart your kernel so the notebook has access to the packages you installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ToQrN8-LcvE7"
      },
      "outputs": [],
      "source": [
        "# Restart the kernel after pip installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "guLbIl38cvE9"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wDz9fPz8cvE-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow_enterprise_addons.cloud_fit import client\n",
        "\n",
        "# Setup and imports\n",
        "REMOTE_DIR = '[gcs-bucket-for-temporary-files]' #@param {type:\"string\"}\n",
        "REGION = 'us-central1' #@param {type:\"string\"}\n",
        "PROJECT_ID = '[your-project-id]' #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID\n",
        "IMAGE_URI = 'gcr.io/{PROJECT_ID}/[name-for-docker-image]:latest' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BTkvsih7cvFA"
      },
      "source": [
        "### Created a docker file with tensorflow_enterprise_addons\n",
        "In the next step we create a base docker file with the latest wheel file to use for remote training. You may use any base image however DLVM base images come pre-installed with most needed packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DYogthoYcvFB"
      },
      "outputs": [],
      "source": [
        "%%file Dockerfile\n",
        "\n",
        "# Using DLVM base image\n",
        "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
        "WORKDIR /root\n",
        "\n",
        "# Path configuration\n",
        "ENV PATH $PATH:/root/tools/google-cloud-sdk/bin\n",
        "\n",
        "# Make sure gsutil will use the default service account\n",
        "RUN echo '[GoogleCompute]\\nservice_account = default' \u003e /etc/boto.cfg\n",
        "\n",
        "# Copy and install tensorflow_enterprise_addons wheel file\n",
        "ADD tensorflow-gcp-tools/python/dist/tensorflow_enterprise_addons-*.whl /tmp/\n",
        "RUN pip3 install --upgrade /tmp/tensorflow_enterprise_addons-*.whl --quiet\n",
        "\n",
        "# Sets up the entry point to invoke cloud_fit.\n",
        "ENTRYPOINT [\"python3\",\"-m\",\"tensorflow_enterprise_addons.cloud_fit.remote\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3XRniIFxcvFE"
      },
      "outputs": [],
      "source": [
        "!docker build -t {IMAGE_URI} -f Dockerfile . -q \u0026\u0026 docker push {IMAGE_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HXzI9doLcvFH"
      },
      "source": [
        "## Tutorial 1 - Functional model\n",
        "In this sample we will demonstrate using numpy.array as input data by creating a basic model and and submit it for remote training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DNo06kD2cvFH"
      },
      "source": [
        "### Define model building function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tOnLcZPHcvFH"
      },
      "outputs": [],
      "source": [
        "\"\"\"Simple model to compute y = wx + 1, with w trainable.\"\"\"\n",
        "inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n",
        "times_w = tf.keras.layers.Dense(\n",
        "  units=1,\n",
        "  kernel_initializer=tf.keras.initializers.Constant([[0.5]]),\n",
        "  kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "  use_bias=False)\n",
        "plus_1 = tf.keras.layers.Dense(\n",
        "  units=1,\n",
        "  kernel_initializer=tf.keras.initializers.Constant([[1.0]]),\n",
        "  bias_initializer=tf.keras.initializers.Constant([1.0]),\n",
        "  trainable=False)\n",
        "outp = plus_1(times_w(inp))\n",
        "simple_model = tf.keras.Model(inp, outp)\n",
        "\n",
        "simple_model.compile(tf.keras.optimizers.SGD(0.002),\n",
        "              \"mean_squared_error\", run_eagerly=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IBqtbBoqcvFK"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UfE9TjpJcvFK"
      },
      "outputs": [],
      "source": [
        "# Creating sample data\n",
        "x = [[9.], [10.], [11.]] * 10\n",
        "y = [[xi[0]/2. + 6] for xi in x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X0KTGKQWcvFN"
      },
      "source": [
        "### Run the model locally for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xPlTCi4EcvFN"
      },
      "outputs": [],
      "source": [
        "# Verify the model by training locally for one step.\n",
        "simple_model.fit(np.array(x), np.array(y), batch_size=len(x), epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XnGy5_8EcvFP"
      },
      "source": [
        "### Submit model and dataset for remote training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WBL9D0LpcvFP"
      },
      "outputs": [],
      "source": [
        "# Create a unique remote sub folder path for assets and model training output.\n",
        "SIMPLE_REMOTE_DIR = os.path.join(REMOTE_DIR, str(uuid.uuid4()))\n",
        "print('your remote folder is %s' % (SIMPLE_REMOTE_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n4vsNr1UcvFS"
      },
      "outputs": [],
      "source": [
        "# Using default configuration with two workers dividing the dataset between the two.\n",
        "simple_model_job_id = client.cloud_fit(model=simple_model, remote_dir = SIMPLE_REMOTE_DIR, region =REGION , image_uri=IMAGE_URI, x=np.array(x), y=np.array(y), epochs=100, steps_per_epoch=len(x)/2,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DwvY8GkScvFV"
      },
      "outputs": [],
      "source": [
        "!gcloud ai-platform jobs describe projects/{PROJECT_ID}/jobs/{simple_model_job_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zdkaGQxncvFY"
      },
      "source": [
        "### Retrieve the trained model\n",
        "Once the training is complete you can access the trained model at remote_folder/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "R3GwKmHscvFY"
      },
      "outputs": [],
      "source": [
        "# Load the trained model from gcs bucket\n",
        "trained_simple_model = tf.keras.models.load_model(os.path.join(SIMPLE_REMOTE_DIR, 'output'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C3tur-zBcvFa"
      },
      "outputs": [],
      "source": [
        "# Test that the saved model loads and works properly\n",
        "trained_simple_model.evaluate(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TLMKhotIcvFc"
      },
      "source": [
        "## Tutorial 2 - Sequential Models and Datasets\n",
        "In this sample we will demonstrate using datasets by creating a basic model and submitting it for remote training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ierzYSPgcvFd"
      },
      "source": [
        "### Define model building function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hds9GY-wcvFd"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "fashion_mnist_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "fashion_mnist_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YL7BFIN1cvFf"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uJB2taSscvFg"
      },
      "outputs": [],
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "images, labels = train\n",
        "images = images/255\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset = dataset.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x2vNQZtBcvFj"
      },
      "source": [
        "### Run the model locally for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nMJGqAAWcvFk"
      },
      "outputs": [],
      "source": [
        "# Verify the model by training locally for one step. This is not necessary prior to cloud.fit() however it is recommended.\n",
        "fashion_mnist_model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9akpewgocvFm"
      },
      "source": [
        "### Submit model and dataset for remote training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XYFj54K1cvFm"
      },
      "outputs": [],
      "source": [
        "# Create a unique remote sub folder path for assets and model training output.\n",
        "FASHION_REMOTE_DIR = os.path.join(REMOTE_DIR, str(uuid.uuid4()))\n",
        "print('your remote folder is %s' % (FASHION_REMOTE_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d25en9VKcvFo"
      },
      "outputs": [],
      "source": [
        "fashion_mnist_model_job_id = client.cloud_fit(model=fashion_mnist_model, remote_dir = FASHION_REMOTE_DIR,region =REGION , image_uri=IMAGE_URI,  x=dataset,epochs=10, steps_per_epoch=15,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4zN6akYkcvFq"
      },
      "outputs": [],
      "source": [
        "!gcloud ai-platform jobs describe projects/{PROJECT_ID}/jobs/{fashion_mnist_model_job_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4I9zkkCkcvFt"
      },
      "source": [
        "### Retrieve the trained model\n",
        "Once the training is complete you can access the trained model at remote_folder/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "unfe2f5NcvFt"
      },
      "outputs": [],
      "source": [
        "# Load the trained model from gcs bucket\n",
        "trained_fashion_mnist_model = tf.keras.models.load_model(os.path.join(FASHION_REMOTE_DIR, 'output'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8gscKRmTcvFv"
      },
      "outputs": [],
      "source": [
        "# Test that the saved model loads and works properly\n",
        "test_images, test_labels = test\n",
        "test_images = test_images/255\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "trained_fashion_mnist_model.evaluate(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cloud_fit.ipynb",
      "provenance": [
        {
          "file_id": "1135EiCC0CKFhSUX5aM38sTFfDxKAmvvo",
          "timestamp": 1593474025992
        }
      ]
    },
    "environment": {
      "name": "tf2-2-2-gpu.2-2.m49",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
